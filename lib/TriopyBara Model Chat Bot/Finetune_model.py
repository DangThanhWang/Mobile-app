# -*- coding: utf-8 -*-
"""Another copy of Fine-tune model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qtGB0ZtRnRnTgNGyMjJZ5okjPQXuMtUq
"""

# Commented out IPython magic to ensure Python compatibility.
# =============================================================================
# LLAMA FACTORY FINE-TUNING PIPELINE
# =============================================================================
# M·ª•c ƒë√≠ch: Fine-tune model nh·ªè tr√™n Google Colab v·ªõi LLaMA Factory
# Author: Optimized Pipeline
# =============================================================================

# =============================================================================
# B∆Ø·ªöC 1: SETUP ENVIRONMENT
# =============================================================================

def setup_environment():
    """Setup LLaMA Factory environment"""
    print("üöÄ Setting up LLaMA Factory environment...")

    # Navigate v√† clean up
#     %cd /content/
#     %rm -rf LLaMA-Factory

    # Clone repository
    print("üì• Cloning LLaMA Factory...")
    !git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git

    # Install dependencies
    print("üì¶ Installing dependencies...")
#     %cd LLaMA-Factory
    !pip install -e .[torch,bitsandbytes] -q

    # Verify GPU
    print("üîç Checking GPU availability...")
    import torch
    try:
        assert torch.cuda.is_available() is True
        print(f"‚úÖ GPU detected: {torch.cuda.get_device_name(0)}")
        print(f"üìä GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    except AssertionError:
        print("‚ùå No GPU detected! Please enable GPU in Colab")
        return False

    return True

# Execute setup
success = setup_environment()
if not success:
    raise RuntimeError("Setup failed - GPU required")

# Commented out IPython magic to ensure Python compatibility.

# =============================================================================
# B∆Ø·ªöC 2: MODEL CONFIGURATION
# =============================================================================

def configure_model():
    """Configure model parameters"""
    print("\nüéØ Configuring model parameters...")

    # Model options for Colab Free
    MODEL_OPTIONS = {
        "qwen2": {
            "name": "unsloth/Qwen2-1.5B-Instruct-bnb-4bit",
            "template": "qwen",
            "output_dir": "qwen2_lora",
            "description": "Qwen2-1.5B (Recommended - best balance)"
        },
        "tinyllama": {
            "name": "unsloth/tinyllama-bnb-4bit",
            "template": "alpaca",
            "output_dir": "tinyllama_lora",
            "description": "TinyLlama-1.1B (Fastest training)"
        },
        "phi3": {
            "name": "unsloth/Phi-3-mini-4k-instruct-bnb-4bit",
            "template": "phi",
            "output_dir": "phi3_lora",
            "description": "Phi-3-Mini-4K (3.8B - slower but better)"
        },
        "gemma": {
            "name": "unsloth/gemma-2b-it-bnb-4bit",
            "template": "gemma",
            "output_dir": "gemma2b_lora",
            "description": "Gemma-2B (Google's model)"
        }
    }

    # Display options
    print("üìã Available models:")
    for key, config in MODEL_OPTIONS.items():
        print(f"   {key}: {config['description']}")

    # Default selection (change if needed)
    SELECTED_MODEL = "qwen2"  # ‚Üê Change this to select different model

    config = MODEL_OPTIONS[SELECTED_MODEL]
    print(f"‚úÖ Selected: {config['description']}")
    print(f"üìù Model: {config['name']}")

    return config

# Configure model
model_config = configure_model()

# =============================================================================
# B∆Ø·ªöC 3: DATASET CUSTOMIZATION
# =============================================================================

def customize_identity_dataset():
    """Customize identity dataset v·ªõi custom information"""
    print("\nüë§ Customizing identity dataset...")

    import json
    import os

    # ‚ö†Ô∏è CUSTOMIZE THESE VALUES
    CUSTOM_NAME = "TriopyBara"        # ‚Üê T√™n AI c·ªßa b·∫°n
    CUSTOM_AUTHOR = "Quang, Tai, Thanh"  # ‚Üê T√™n t√°c gi·∫£

    print(f"üìù AI Name: {CUSTOM_NAME}")
    print(f"üë®‚Äçüíª Author: {CUSTOM_AUTHOR}")

    # Load v√† modify dataset
    dataset_path = "data/identity.json"

    if not os.path.exists(dataset_path):
        print(f"‚ùå Dataset not found: {dataset_path}")
        return False

    try:
        with open(dataset_path, "r", encoding="utf-8") as f:
            dataset = json.load(f)

        # Replace placeholders
        for sample in dataset:
            sample["output"] = sample["output"].replace("{{name}}", CUSTOM_NAME)
            sample["output"] = sample["output"].replace("{{author}}", CUSTOM_AUTHOR)

        # Save modified dataset
        with open(dataset_path, "w", encoding="utf-8") as f:
            json.dump(dataset, f, indent=2, ensure_ascii=False)

        print(f"‚úÖ Identity dataset customized ({len(dataset)} samples)")
        return True

    except Exception as e:
        print(f"‚ùå Error customizing dataset: {e}")
        return False

# Customize dataset
dataset_success = customize_identity_dataset()

# =============================================================================
# B∆Ø·ªöC 4: TRAINING CONFIGURATION
# =============================================================================

def create_training_config(model_config):
    """Create optimized training configuration"""
    print("\n‚öôÔ∏è Creating training configuration...")

    import json

    # Optimized config cho Colab Free
    training_args = {
        # Basic settings
        "stage": "sft",
        "do_train": True,
        "model_name_or_path": model_config["name"],
        "template": model_config["template"],
        "output_dir": model_config["output_dir"],

        # Dataset settings
        "dataset": "identity,identity3,claude_data2,simple_data",  # Repeat for emphasis
        "max_samples": 1000,

        # LoRA settings
        "finetuning_type": "lora",
        "lora_target": "all",
        "loraplus_lr_ratio": 16.0,

        # Training hyperparameters
        "learning_rate": 1e-4,
        "num_train_epochs": 6.0,
        "per_device_train_batch_size": 4,
        "gradient_accumulation_steps": 2,
        "max_grad_norm": 1.0,

        # Optimization
        "lr_scheduler_type": "cosine",
        "warmup_ratio": 0.1,
        "fp16": True,

        # Logging & Saving
        "logging_steps": 10,
        "save_steps": 500,
        "save_total_limit": 2,
        "report_to": "none",

        # Memory optimization
        "dataloader_pin_memory": False,
        "gradient_checkpointing": True
    }

    # Save config
    config_file = "optimized_training.json"
    with open(config_file, "w", encoding="utf-8") as f:
        json.dump(training_args, f, indent=2)

    print(f"‚úÖ Training config saved: {config_file}")
    print(f"üìä Expected training time: 20-30 minutes")
    print(f"üíæ Model size after training: ~35MB (LoRA only)")

    return config_file, training_args

# Create training config
config_file, training_args = create_training_config(model_config)

# =============================================================================
# B∆Ø·ªöC 5: EXECUTE TRAINING
# =============================================================================

def start_training(config_file):
    """Start the training process"""
    print("\nüöÄ Starting training process...")
    print("="*60)

    import time
    start_time = time.time()

    # Navigate to correct directory
#     %cd /content/LLaMA-Factory/

    # Start training
    print("üî• Training started - this will take 20-30 minutes...")
    !llamafactory-cli train {config_file}

    # Calculate training time
    end_time = time.time()
    training_duration = (end_time - start_time) / 60

    print("="*60)
    print(f"‚è±Ô∏è Training completed in {training_duration:.1f} minutes")

    return training_duration

# Execute training
training_time = start_training(config_file)

# =============================================================================
# B∆Ø·ªöC 6: VERIFY TRAINING RESULTS
# =============================================================================

def verify_training_results(output_dir):
    """Verify training results v√† ki·ªÉm tra files"""
    print("\nüîç Verifying training results...")

    import os

    if not os.path.exists(output_dir):
        print(f"‚ùå Output directory not found: {output_dir}")
        return False

    # Check essential files
    essential_files = [
        "adapter_model.safetensors",
        "adapter_config.json",
        "tokenizer_config.json"
    ]

    print(f"üìÅ Checking files in {output_dir}:")
    total_size = 0

    for file in os.listdir(output_dir):
        file_path = os.path.join(output_dir, file)
        size_mb = os.path.getsize(file_path) / 1024 / 1024
        total_size += size_mb

        status = "‚úÖ" if file in essential_files else "üìÑ"
        print(f"   {status} {file}: {size_mb:.1f} MB")

    print(f"üìä Total size: {total_size:.1f} MB")

    # Check if all essential files exist
    missing_files = [f for f in essential_files if not os.path.exists(os.path.join(output_dir, f))]

    if missing_files:
        print(f"‚ö†Ô∏è Missing files: {missing_files}")
        return False
    else:
        print("‚úÖ All essential files present!")
        return True

def quick_functionality_test():
    """Quick test ƒë·ªÉ ƒë·∫£m b·∫£o imports work"""
    print("\nüß™ Quick functionality test...")

    try:
        # Test imports
        from transformers import AutoTokenizer
        print("‚úÖ transformers import successful")

        # Test tokenizer loading
        tokenizer = AutoTokenizer.from_pretrained(model_config["name"])
        print("‚úÖ Tokenizer loading successful")

        # Test tokenization
        test_input = "Hello, how are you?"
        tokens = tokenizer(test_input, return_tensors="pt")
        print(f"‚úÖ Tokenization successful: {len(tokens['input_ids'][0])} tokens")

        return True

    except Exception as e:
        print(f"‚ö†Ô∏è Test failed: {e}")
        print("üí° This is usually fine - model still trained successfully")
        return False

# Verify results
verification_success = verify_training_results(model_config["output_dir"])
test_success = quick_functionality_test()

# =============================================================================
# B∆Ø·ªöC 7: PACKAGE V√Ä DOWNLOAD
# =============================================================================

def create_deployment_package(output_dir, model_name):
    """Create complete deployment package"""
    print("\nüì¶ Creating deployment package...")

    import shutil
    import os
    from google.colab import files

    # Create package directory
    package_dir = "/content/finetuned_model_package"
    if os.path.exists(package_dir):
        shutil.rmtree(package_dir)
    os.makedirs(package_dir)

    # Copy model files
    print("üìÅ Copying model files...")
    shutil.copytree(output_dir, package_dir, dirs_exist_ok=True)

    # Create comprehensive README
    readme_content = f"""# ü§ñ Fine-tuned {model_name} Model

## Model Information
- **Base Model**: {model_config['name']}
- **Fine-tuning Method**: LoRA (Low-Rank Adaptation)
- **Training Time**: {training_time:.1f} minutes
- **Template**: {model_config['template']}
- **Size**: ~35MB (LoRA adapters only)

## Quick Start

### Installation
```bash
pip install torch transformers peft accelerate safetensors
```

### Usage
```python
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

# Load base model
model_name = "{model_config['name'].replace('unsloth/', '').replace('-bnb-4bit', '')}"
tokenizer = AutoTokenizer.from_pretrained(model_name)
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16,
    device_map="auto"
)

# Load LoRA adapter
model = PeftModel.from_pretrained(base_model, "./")
model = model.merge_and_unload()  # Optional: merge weights

# Chat function
def chat(question):
    prompt = f"<|im_start|>user\\n{{question}}<|im_end|>\\n<|im_start|>assistant\\n"
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=150,
            do_sample=True,
            temperature=0.7,
            pad_token_id=tokenizer.eos_token_id
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response[len(prompt):].strip()

# Test
print(chat("Hello, who are you?"))
```

## Files Included
- `adapter_model.safetensors`: LoRA weights
- `adapter_config.json`: LoRA configuration
- `tokenizer files`: For text processing
- `test_model.py`: Ready-to-run test script
- `requirements.txt`: Dependencies

## Performance Tips
- Use temperature 0.7 for balanced responses
- GPU recommended for faster inference
- Batch processing for multiple queries

## Support
This model was trained using LLaMA Factory on Google Colab.
For issues, check the test script or retrain if needed.
"""

    with open(os.path.join(package_dir, "README.md"), "w", encoding="utf-8") as f:
        f.write(readme_content)

    # Create test script
    test_script = f'''"""
Test script for fine-tuned {model_name} model
"""
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

def test_model():
    print("üöÄ Loading fine-tuned model...")

    # Model names (adjust if needed)
    base_model_name = "{model_config['name'].replace('unsloth/', '').replace('-bnb-4bit', '')}"

    try:
        # Load tokenizer v√† base model
        tokenizer = AutoTokenizer.from_pretrained(base_model_name)
        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )

        # Load LoRA adapter
        model = PeftModel.from_pretrained(base_model, "./")
        print("‚úÖ Model loaded successfully!")

        # Test questions
        questions = [
            "Hello, who are you?",
            "Explain artificial intelligence",
            "Write a Python function to sort a list"
        ]

        print("\\nüß™ Testing model responses...")
        for i, question in enumerate(questions, 1):
            print(f"\\n{{i}}. Q: {{question}}")

            prompt = f"<|im_start|>user\\n{{question}}<|im_end|>\\n<|im_start|>assistant\\n"
            inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=100,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=tokenizer.eos_token_id
                )

            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            answer = response[len(prompt):].strip()
            print(f"   A: {{answer}}")

        print("\\n‚úÖ All tests completed successfully!")

    except Exception as e:
        print(f"‚ùå Error: {{e}}")
        print("üí° Make sure you have installed: pip install -r requirements.txt")

if __name__ == "__main__":
    test_model()
'''

    with open(os.path.join(package_dir, "test_model.py"), "w") as f:
        f.write(test_script)

    # Create requirements
    requirements = """torch>=2.0.0
transformers>=4.44.0
peft>=0.6.0
accelerate>=0.20.0
safetensors>=0.3.0
"""
    with open(os.path.join(package_dir, "requirements.txt"), "w") as f:
        f.write(requirements)

    # Create ZIP package
    print("üóúÔ∏è Creating ZIP archive...")
    zip_path = "/content/finetuned_model_complete"
    shutil.make_archive(zip_path, "zip", package_dir)

    # Get package info
    zip_size = os.path.getsize(f"{zip_path}.zip") / 1024 / 1024
    print(f"‚úÖ Package created: {zip_size:.1f} MB")

    return f"{zip_path}.zip"

def download_package(zip_path):
    """Download the complete package"""
    print("\nüì• Starting download...")

    from google.colab import files
    files.download(zip_path)

    print("\nüéâ DOWNLOAD COMPLETED!")
    print("\nüìã Package includes:")
    print("   ‚úÖ LoRA adapter weights")
    print("   ‚úÖ Configuration files")
    print("   ‚úÖ Usage documentation")
    print("   ‚úÖ Test script")
    print("   ‚úÖ Requirements file")

    print("\nüöÄ Next steps:")
    print("   1. Extract ZIP file")
    print("   2. Install requirements: pip install -r requirements.txt")
    print("   3. Run test: python test_model.py")
    print("   4. Use in your applications!")

# Create v√† download package
if verification_success:
    package_path = create_deployment_package(model_config["output_dir"], model_config["name"])
    download_package(package_path)
else:
    print("‚ùå Cannot create package - training verification failed")

# =============================================================================
# PIPELINE SUMMARY
# =============================================================================

print("\n" + "="*60)
print("üèÅ TRAINING PIPELINE COMPLETED")
print("="*60)

print(f"‚úÖ Model: {model_config['name']}")
print(f"‚úÖ Training time: {training_time:.1f} minutes")
print(f"‚úÖ Output directory: {model_config['output_dir']}")
print(f"‚úÖ Files verified: {'Yes' if verification_success else 'No'}")
print(f"‚úÖ Package created: {'Yes' if verification_success else 'No'}")

if verification_success:
    print("\nüéØ Your fine-tuned model is ready to use!")
    print("üì¶ All files downloaded successfully")
    print("üöÄ Deploy anywhere with the provided scripts")
else:
    print("\n‚ö†Ô∏è Some verification steps failed")
    print("üí° Model may still be usable - check files manually")

print("\nüí° Tips for next use:")
print("   - Model works with transformers + PEFT")
print("   - Supports both Vietnamese and English")
print("   - Use provided test script for quick validation")
print("   - Memory requirement: ~6GB VRAM for inference")

print("\nüîÑ To retrain: Run this notebook again")
print("="*60)