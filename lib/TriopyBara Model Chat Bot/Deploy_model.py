# -*- coding: utf-8 -*-
"""Copy of Deploy model fine-tuned.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ROTYpq0E1qsXwoVwYMGDSWRTT4i6uEXh
"""

# =============================================================================
# FASTAPI + QWEN2-1.5B FINE-TUNED CHATBOT API
# =============================================================================
# Deploy fine-tuned Qwen2-1.5B model v·ªõi FastAPI cho mobile app
# GPU acceleration + LoRA adapters + Public API
# =============================================================================

# =============================================================================
# B∆Ø·ªöC 1: C√ÄI ƒê·∫∂T PACKAGES
# =============================================================================

print("üì¶ Installing required packages...")
!pip install torch transformers peft fastapi uvicorn pyngrok accelerate safetensors -q
print("‚úÖ All packages installed successfully!")

# =============================================================================
# B∆Ø·ªöC 2: SETUP MODEL T·ª™ GOOGLE DRIVE
# =============================================================================

from google.colab import drive
import shutil
import os
import json
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import time

# Global variables cho model
model = None
tokenizer = None
model_loading_status = {"loaded": False, "error": None}

def setup_model_from_drive():
    """Upload model t·ª´ Google Drive"""
    print("üìÅ Setting up model from Google Drive...")

    # Mount Google Drive
    print("üîó Mounting Google Drive...")
    drive.mount('/content/drive')
    print("‚úÖ Google Drive mounted successfully!")

    # ‚ö†Ô∏è THAY ƒê·ªîI PATH N√ÄY THEO V·ªä TR√ç MODEL C·ª¶A B·∫†N
    source_path = '/content/drive/MyDrive/Chatbot7'
    dest_path = '/content/model_files'

    print(f"üìÇ Looking for model at: {source_path}")

    if os.path.exists(source_path):
        print("üìã Copying model files...")
        if os.path.exists(dest_path):
            shutil.rmtree(dest_path)
        shutil.copytree(source_path, dest_path)

        # Ki·ªÉm tra files ƒë√£ copy
        print("\nüìã Model files copied:")
        for root, dirs, files in os.walk(dest_path):
            for file in files:
                file_path = os.path.join(root, file)
                size = os.path.getsize(file_path)
                print(f"  ‚úÖ {file} ({size:,} bytes)")

        return dest_path
    else:
        print(f"‚ùå Model folder not found at: {source_path}")
        return None

def load_model_with_adapter():
    """Load model v·ªõi LoRA adapter v√† GPU acceleration"""
    global model, tokenizer, model_loading_status

    if model is not None:
        print("‚úÖ Model already loaded")
        return model, tokenizer

    print("üöÄ Loading Qwen2-1.5B with GPU acceleration...")
    start_time = time.time()

    try:
        model_name = "Qwen/Qwen2-1.5B-Instruct"

        # Load tokenizer
        print("üìù Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)

        # Load base model v·ªõi GPU
        print("üß† Loading base model...")
        base_model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )

        # Load LoRA adapter n·∫øu c√≥
        model_path = setup_model_from_drive()
        if model_path and os.path.exists(model_path):
            adapter_model_file = os.path.join(model_path, 'adapter_model.safetensors')
            adapter_config_file = os.path.join(model_path, 'adapter_config.json')

            if os.path.exists(adapter_model_file) and os.path.exists(adapter_config_file):
                print(f"üìÅ Loading LoRA adapter from: {model_path}")

                # Load v√† merge adapter
                model = PeftModel.from_pretrained(base_model, model_path)
                model = model.merge_and_unload()
                print("‚úÖ LoRA adapter loaded and merged!")
            else:
                print("‚ö†Ô∏è Adapter files not found, using base model")
                model = base_model
        else:
            print("‚ö†Ô∏è No model path, using base model")
            model = base_model

        # Ki·ªÉm tra device
        device = next(model.parameters()).device
        load_time = time.time() - start_time

        print(f"‚úÖ Model loaded on {device} in {load_time:.1f}s")

        # Update status
        model_loading_status["loaded"] = True
        model_loading_status["error"] = None

        return model, tokenizer

    except Exception as e:
        error_msg = f"Error loading model: {e}"
        print(f"‚ùå {error_msg}")
        model_loading_status["loaded"] = False
        model_loading_status["error"] = error_msg
        return None, None

def generate_chat_response(message, max_tokens=200, temperature=0.7):
    """Generate response t·ª´ model"""
    global model, tokenizer

    if model is None or tokenizer is None:
        return "‚ùå Model ch∆∞a ƒë∆∞·ª£c load. Vui l√≤ng ƒë·ª£i ho·∫∑c th·ª≠ l·∫°i."

    try:
        start_time = time.time()

        # Create prompt
        prompt = f"<|im_start|>user\n{message}<|im_end|>\n<|im_start|>assistant\n"

        # Generate response
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                do_sample=True,
                temperature=temperature,
                top_p=0.9,
                pad_token_id=tokenizer.eos_token_id
            )

        # Decode response
        response_ids = outputs[0][inputs["input_ids"].shape[-1]:]
        answer = tokenizer.decode(response_ids, skip_special_tokens=True).strip()

        # Clean response
        if "<|im_end|>" in answer:
            answer = answer.split("<|im_end|>")[0].strip()

        generation_time = time.time() - start_time

        return {
            "response": answer,
            "generation_time": generation_time,
            "model_info": "Qwen2-1.5B Fine-tuned"
        }

    except Exception as e:
        return {
            "response": f"‚ùå L·ªói generate: {str(e)}",
            "generation_time": 0,
            "error": str(e)
        }

# =============================================================================
# B∆Ø·ªöC 3: FASTAPI SERVER
# =============================================================================

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional
import asyncio
import threading

# Create FastAPI app
app = FastAPI(
    title="ü§ñ Qwen2-1.5B Fine-tuned Chat API",
    description="Fine-tuned Vietnamese chatbot API v·ªõi GPU acceleration",
    version="2.0.0"
)

# Enable CORS cho mobile app
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request/Response models
class ChatRequest(BaseModel):
    message: str
    user_id: str = "anonymous"
    max_tokens: Optional[int] = 200
    temperature: Optional[float] = 0.7

class ChatResponse(BaseModel):
    response: str
    generation_time: float
    model_info: str
    user_id: str
    status: str = "success"

class ModelStatus(BaseModel):
    loaded: bool
    error: Optional[str] = None
    model_name: str = "Qwen2-1.5B Fine-tuned"
    device: Optional[str] = None

# API Endpoints
@app.get("/")
async def root():
    return {
        "message": "ü§ñ Qwen2-1.5B Fine-tuned Chat API",
        "status": "online",
        "model_loaded": model_loading_status["loaded"],
        "endpoints": ["/chat", "/health", "/model-status", "/docs"]
    }

@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": time.time(),
        "model_loaded": model_loading_status["loaded"],
        "gpu_available": torch.cuda.is_available()
    }

@app.get("/model-status", response_model=ModelStatus)
async def get_model_status():
    device_info = None
    if model is not None:
        device_info = str(next(model.parameters()).device)

    return ModelStatus(
        loaded=model_loading_status["loaded"],
        error=model_loading_status["error"],
        device=device_info
    )

@app.post("/chat", response_model=ChatResponse)
async def chat_endpoint(request: ChatRequest):
    """Main chat endpoint"""
    try:
        # Validate input
        if not request.message.strip():
            raise HTTPException(status_code=400, detail="Message cannot be empty")

        # Check model status
        if not model_loading_status["loaded"]:
            raise HTTPException(
                status_code=503,
                detail=f"Model not ready: {model_loading_status.get('error', 'Loading...')}"
            )

        # Generate response
        result = generate_chat_response(
            request.message,
            request.max_tokens,
            request.temperature
        )

        if "error" in result:
            raise HTTPException(status_code=500, detail=result["error"])

        return ChatResponse(
            response=result["response"],
            generation_time=result["generation_time"],
            model_info=result["model_info"],
            user_id=request.user_id
        )

    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Internal error: {str(e)}")

@app.post("/chat/quick")
async def quick_chat(message: str):
    """Quick chat endpoint v·ªõi parameters m·∫∑c ƒë·ªãnh"""
    request = ChatRequest(message=message)
    return await chat_endpoint(request)

@app.post("/reload-model")
async def reload_model():
    """Reload model (for debugging)"""
    global model, tokenizer, model_loading_status

    model = None
    tokenizer = None
    model_loading_status = {"loaded": False, "error": None}

    # Reload trong background
    def reload():
        load_model_with_adapter()

    thread = threading.Thread(target=reload)
    thread.daemon = True
    thread.start()

    return {"message": "Model reload started", "status": "reloading"}

# =============================================================================
# B∆Ø·ªöC 4: NGROK SETUP V√Ä DEPLOYMENT
# =============================================================================

from pyngrok import ngrok
import uvicorn

def run_server():
    """Ch·∫°y FastAPI server"""
    uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")

def deploy_api():
    """Deploy API v·ªõi ngrok"""

    # Setup ngrok
    print("üîß Setting up ngrok...")
    ngrok.set_auth_token("2y8dZhOGOL9ZURNdbDgzAoiH5OZ_7aP6GRPQuYsoGqrRXE4h7")
    ngrok.kill()

    # Start server trong thread
    print("üöÄ Starting FastAPI server...")
    server_thread = threading.Thread(target=run_server)
    server_thread.daemon = True
    server_thread.start()

    # ƒê·ª£i server kh·ªüi ƒë·ªông
    time.sleep(4)

    # T·∫°o ngrok tunnel
    print("üåê Creating public tunnel...")
    public_url = ngrok.connect(8000)

    print(f"\nüéâ API DEPLOYED SUCCESSFULLY!")
    print("="*60)
    print(f"üåê Base URL: {public_url}")
    print(f"ü§ñ Chat API: {public_url}/chat")
    print(f"‚ö° Quick Chat: {public_url}/chat/quick")
    print(f"‚ù§Ô∏è Health Check: {public_url}/health")
    print(f"üìä Model Status: {public_url}/model-status")
    print(f"üìö API Docs: {public_url}/docs")
    print("="*60)

    print("\nüì± MOBILE APP INTEGRATION:")
    print("="*40)
    print(f"const API_BASE = '{public_url}';")
    print("// POST /chat")
    print('// {"message": "Hello", "user_id": "mobile_user"}')

    print(f"\nüìã CURL TEST:")
    print(f'curl -X POST "{public_url}/chat" \\')
    print('     -H "Content-Type: application/json" \\')
    print('     -d \'{"message": "Xin ch√†o!", "user_id": "test"}\'')

    return public_url

# =============================================================================
# B∆Ø·ªöC 5: EXECUTION
# =============================================================================

print("\n" + "="*60)
print("üöÄ STARTING QWEN2-1.5B FASTAPI DEPLOYMENT")
print("="*60)

# Load model tr∆∞·ªõc
print("\n1Ô∏è‚É£ Loading model...")
model, tokenizer = load_model_with_adapter()

if model is not None:
    print("\n2Ô∏è‚É£ Model loaded successfully! Starting API...")

    # Warm up model
    print("üî• Warming up model...")
    warmup_result = generate_chat_response("Hello")
    print(f"   ‚úÖ Warmup: {warmup_result['response'][:50]}...")

    # Deploy API
    print("\n3Ô∏è‚É£ Deploying API...")
    api_url = deploy_api()

    print(f"\nüéØ READY TO USE!")
    print(f"üîó API URL: {api_url}")
    print("üì± Mobile app c√≥ th·ªÉ connect ngay!")

    # Keep server running
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\nüëã Server stopped")
        ngrok.kill()

else:
    print("‚ùå Model loading failed!")
    print("üîß Please check your Google Drive path v√† th·ª≠ l·∫°i")

print("\nüèÅ Deployment completed!")